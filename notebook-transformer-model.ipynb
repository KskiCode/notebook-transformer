{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2-large\") # 774M parameters\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use the end-of-sequence token as padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding (Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self,\n",
    "                token_embeddings_q,\n",
    "                token_embeddings_k,\n",
    "                token_embeddings_v,\n",
    "                mask=None):\n",
    "        \n",
    "        # [batch, seq_len, d_model]\n",
    "        q = self.W_q(token_embeddings_q)  \n",
    "        k = self.W_k(token_embeddings_k)  \n",
    "        v = self.W_v(token_embeddings_v)  \n",
    "\n",
    "        # For batched inputs, we need to transpose the last two dimensions\n",
    "        # This keeps the batch dimension (dim 0) intact!\n",
    "        sims = torch.matmul(q, k.transpose(-2, -1))  # [batch, seq_len, seq_len]\n",
    "\n",
    "        scaled_sims = sims / math.sqrt(k.size(-1)) # scaled dot product\n",
    "\n",
    "        if mask is not None:\n",
    "            scaled_sims = scaled_sims.masked_fill(mask=mask, value=-1e9)\n",
    "        \n",
    "        attention_percents = F.softmax(scaled_sims, dim=-1)  # Apply softmax along the last dimension\n",
    "\n",
    "        attention_scores = torch.matmul(attention_percents, v)\n",
    "\n",
    "        return attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList(\n",
    "            [Attention(d_model=d_model, dropout=dropout)\n",
    "            for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "        print(self.heads)\n",
    "\n",
    "        # Add projection layer to combine outputs from multiple heads\n",
    "        self.output_projection = nn.Linear(d_model * num_heads, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, \n",
    "                token_embeddings_q,\n",
    "                token_embeddings_k,\n",
    "                token_embeddings_v,\n",
    "                mask=None):\n",
    "        \n",
    "        # Concatenate outputs from all attention heads\n",
    "        multi_head_output = torch.cat(\n",
    "            [head(token_embeddings_q, token_embeddings_k, token_embeddings_v, mask)\n",
    "            for head in self.heads],\n",
    "            dim=-1)\n",
    "        \n",
    "        output = self.output_projection(multi_head_output)\n",
    "        \n",
    "        return self.dropout(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed Forward Neural Network (FFNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x is the output of the multi-head attention layer - tensor of shape [batch, seq_len, d_model*num_heads]\"\"\"\n",
    "     \n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attn_output = self.attention(x, x, x, mask)\n",
    "        x = self.norm1(x + attn_output)\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, num_layers=6, d_ff=2048, max_seq_len=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embedding layer\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Get token embeddings\n",
    "        x = self.token_embedding(x)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        # Apply final layer norm\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        logits = self.output_projection(x)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def generate(self, input_ids, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate text by sampling from the model's distribution.\n",
    "        \n",
    "        Args:\n",
    "            input_ids (Tensor): Starting token indices of shape [batch_size, seq_len]\n",
    "            max_new_tokens (int): Maximum number of new tokens to generate\n",
    "            temperature (float): Temperature for sampling (higher = more random)\n",
    "            top_k (int, optional): If specified, only sample from the top k most likely tokens\n",
    "            \n",
    "        Returns:\n",
    "            Tensor: Generated token indices of shape [batch_size, seq_len + max_new_tokens]\n",
    "        \"\"\"\n",
    "        self.eval()  # Set to evaluation mode\n",
    "        \n",
    "        # Create a copy of the input tensor to avoid modifying the original\n",
    "        generated_ids = input_ids.clone()\n",
    "        batch_size = generated_ids.size(0)\n",
    "        \n",
    "        # Create causal mask for the maximum possible sequence length\n",
    "        seq_len = generated_ids.size(1)\n",
    "        max_possible_len = seq_len + max_new_tokens\n",
    "        causal_mask = torch.triu(\n",
    "            torch.ones(max_possible_len, max_possible_len), diagonal=1\n",
    "        ).bool().to(input_ids.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                # If sequence is too long, truncate it to fit model's context window\n",
    "                if generated_ids.size(1) > 1024:  # Assuming 1024 is the max context length\n",
    "                    generated_ids = generated_ids[:, -1024:]\n",
    "                \n",
    "                # Get the current sequence length\n",
    "                curr_seq_len = generated_ids.size(1)\n",
    "                \n",
    "                # Use the appropriate part of the causal mask\n",
    "                curr_mask = causal_mask[:curr_seq_len, :curr_seq_len].unsqueeze(0)\n",
    "                \n",
    "                # Forward pass to get logits\n",
    "                logits = self(generated_ids, mask=curr_mask)\n",
    "                \n",
    "                # Get the logits for the next token prediction (last token in sequence)\n",
    "                next_token_logits = logits[:, -1, :] / temperature\n",
    "                \n",
    "                # Optional top-k sampling\n",
    "                if top_k is not None:\n",
    "                    top_k = min(top_k, next_token_logits.size(-1))\n",
    "                    # Get the top-k values and indices\n",
    "                    values, indices = torch.topk(next_token_logits, top_k, dim=-1)\n",
    "                    # Create a mask for the top-k values\n",
    "                    mask = torch.zeros_like(next_token_logits).scatter_(1, indices, 1)\n",
    "                    # Apply the mask and set non-top-k values to -inf\n",
    "                    next_token_logits = torch.where(mask.bool(), next_token_logits, \n",
    "                                                   torch.tensor(-float('inf')).to(next_token_logits.device))\n",
    "                \n",
    "                # Apply softmax to get probabilities\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                \n",
    "                # Sample from the distribution\n",
    "                next_token = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                # Append the sampled token to the sequence\n",
    "                generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "                \n",
    "        return generated_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = NotebookGPT(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    num_layers=4,\n",
    "    d_ff=2048,\n",
    "    dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, tokenizer, train_data, epochs=3, lr=5e-5, batch_size=4):\n",
    "    \"\"\"\n",
    "    Simple training loop for the NotebookGPT model.\n",
    "    \n",
    "    Args:\n",
    "        model: The NotebookGPT model\n",
    "        tokenizer: The tokenizer\n",
    "        train_data: List of text samples for training\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        batch_size: Batch size\n",
    "    \"\"\"\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Adjust batch size if it's larger than the dataset\n",
    "    batch_size = min(batch_size, len(train_data))\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Define loss function\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # Process data in batches\n",
    "        for i in range(0, len(train_data), batch_size):\n",
    "            batch_texts = train_data[i:i+batch_size]\n",
    "            \n",
    "            # Tokenize batch\n",
    "            inputs = tokenizer(batch_texts, padding=True, truncation=True, \n",
    "                              return_tensors=\"pt\", max_length=512)\n",
    "            input_ids = inputs.input_ids.to(device)\n",
    "            \n",
    "            # Create targets (shifted input_ids)\n",
    "            targets = input_ids.clone()\n",
    "            \n",
    "            # Create causal mask\n",
    "            seq_len = input_ids.size(1)\n",
    "            causal_mask = torch.triu(\n",
    "                torch.ones(seq_len, seq_len), diagonal=1\n",
    "            ).bool().to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids, mask=causal_mask)\n",
    "            \n",
    "            # Reshape for loss calculation\n",
    "            logits = logits[:, :-1, :].contiguous().view(-1, tokenizer.vocab_size)\n",
    "            targets = targets[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(logits, targets)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "        # Print epoch statistics\n",
    "        avg_loss = total_loss / max(1, batch_count)  # Avoid division by zero\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from file\n",
    "with open(r'data\\training-data.txt', 'r', encoding='utf-8') as f:\n",
    "    train_data = f.readlines()\n",
    "    \n",
    "# Remove any empty lines and strip whitespace\n",
    "train_data = [line.strip() for line in train_data if line.strip()]\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = train_model(model, tokenizer, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"The story of the Karamozovs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input (text to token)\n",
    "prompt_input_tokens = tokenizer.encode(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = prompt_input_tokens.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text\n",
    "generated_ids = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.1,\n",
    "    top_k=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids # the sequence of tokens the model generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count the total number of trainable parameters in the model.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        \n",
    "    Returns:\n",
    "        int: Total number of trainable parameters\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Count and display the number of parameters\n",
    "total_params = count_parameters(model)\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "\n",
    "# You can also print a more detailed breakdown by layer\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.numel():,} parameters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LlmPlaygroundENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
